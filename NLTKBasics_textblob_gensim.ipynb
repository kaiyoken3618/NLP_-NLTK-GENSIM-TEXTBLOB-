{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTKBasics_textblob_gensim.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiyoken3618/NLP_-NLTK-GENSIM-TEXTBLOB-/blob/main/NLTKBasics_textblob_gensim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q3lPFMfaFN_"
      },
      "source": [
        "Install NLTK packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0qih0aOYIEC"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9jGBYTOZ7ga"
      },
      "source": [
        "Noise Remove Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9CYhn70YZ6uB",
        "outputId": "e59fd367-0998-47ff-fe7b-2ccbed68d093"
      },
      "source": [
        "noise_list = [\"is\", \"a\", \"this\",\"of\",\"that\",\"there\",\"to\",\"#\"] \n",
        "def _remove_noise(input_text):\n",
        "    words = input_text.split() \n",
        "    noise_free_words = [word for word in words if word not in noise_list] \n",
        "    noise_free_text = \" \".join(noise_free_words) \n",
        "    return noise_free_text\n",
        "\n",
        "_remove_noise(\"I cannot # connect to SFTP\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I cannot connect SFTP'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grW3-IVldjI0"
      },
      "source": [
        "Regex (It is used to remove regular expression) -- Still unclear  \n",
        "**Alternate of noise remove test over this block**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nuLBu46RegpD",
        "outputId": "beb08f06-324a-4131-d2c2-e214037db39d"
      },
      "source": [
        "import re \n",
        "\n",
        "def _remove_regex(input_text, regex_pattern):\n",
        "    urls = re.finditer(regex_pattern, input_text) \n",
        "    for i in urls: \n",
        "        input_text = re.sub(i.group().strip(), '', input_text)\n",
        "    return input_text\n",
        "\n",
        "regex_pattern = \"#[\\w]*\"  \n",
        "\n",
        "_remove_regex(\"I cannot # connect to SFTP\", regex_pattern)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I cannot  connect to SFTP'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fujwR5erlBYP"
      },
      "source": [
        "(Lexical Normalization)Stemming and lemmatization  \n",
        "Lemmatization will work better with POS-tag.\n",
        " \n",
        "\n",
        "*   (lemma_Example: multiplying ; output: multiply)\n",
        "*   (stem_Example: multiplying; output: multi)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA9fq-4Zk_cv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0afff6b9-dee8-4348-c0ad-525b3695653b"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "lem = WordNetLemmatizer() \n",
        "word = \"multiplying\" \n",
        "lem.lemmatize(word,\"v\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'multiply'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e_0LYuJxTXwZ",
        "outputId": "1ed81014-4982-4df9-f2fc-0eed951d0668"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer \n",
        "stem = PorterStemmer()  \n",
        "stem.stem(word)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'multipli'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P7XWw5HMTlq"
      },
      "source": [
        "Object stadardization > Removing acronyms, shortcuts that excludes Lexical Dictionaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fdAPtkz-P5Uj",
        "outputId": "73f17ecc-d2f6-4211-da6c-d25937561e9a"
      },
      "source": [
        "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\", \"k\" : \"ok\"}\n",
        "def _lookup_words(input_text):\n",
        "    words = input_text.split() \n",
        "    new_words = [] \n",
        "    for word in words:\n",
        "        if word.lower() in lookup_dict:\n",
        "            word = lookup_dict[word.lower()]\n",
        "            new_words.append(word) \n",
        "            new_text = \" \".join(new_words) \n",
        "        return new_text\n",
        "\n",
        "_lookup_words(\"rt this is a retweeted tweet by Shivam Bansal \") \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Retweet'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny1IoVCtW9AM"
      },
      "source": [
        "Parts of speech (POS - tags) > https://pythonspot.com/nltk-speech-tagging/ \n",
        "Application: \n",
        "\n",
        "\n",
        "*   Word sense disambiguation \n",
        "*   Improving word-based features \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHiPNywcXAqu",
        "outputId": "623dca0c-3eb8-4e6e-a6e3-74f4cac37b33"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
        "tokens = word_tokenize(text)\n",
        "print (pos_tag(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVZcoLz_kQ75"
      },
      "source": [
        "Entity Extraction(Topic Modeling) -> To learn more Need to go through\n",
        " https://monkeylearn.com/blog/introduction-to-topic-modeling/#:~:text=Topic%20modeling%20is%20an%20unsupervised,characterize%20a%20set%20of%20documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC0kbZGmkW0j",
        "outputId": "d116bbf3-94eb-44e6-d414-aed71b2a6cc2"
      },
      "source": [
        "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
        "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
        "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
        "doc_complete = [doc1, doc2, doc3]\n",
        "doc_clean = [doc.split() for doc in doc_complete]\n",
        " \n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Training LDA model on the document term matrix\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
        "\n",
        "# Results \n",
        "print(ldamodel.print_topics())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.083*\"to\" + 0.058*\"My\" + 0.058*\"sister\" + 0.058*\"my\" + 0.033*\"likes\" + 0.033*\"sugar,\" + 0.033*\"not\" + 0.033*\"Sugar\" + 0.033*\"but\" + 0.033*\"have\"'), (1, '0.060*\"driving\" + 0.060*\"Doctors\" + 0.060*\"cause\" + 0.060*\"and\" + 0.060*\"that\" + 0.060*\"blood\" + 0.060*\"increased\" + 0.060*\"stress\" + 0.060*\"suggest\" + 0.060*\"may\"'), (2, '0.029*\"driving\" + 0.029*\"dance\" + 0.029*\"time\" + 0.029*\"father\" + 0.029*\"around\" + 0.029*\"a\" + 0.029*\"practice.\" + 0.029*\"spends\" + 0.029*\"of\" + 0.029*\"lot\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6yOBVxpmmT"
      },
      "source": [
        "N-Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzpeJxh2pn7P",
        "outputId": "0f8597e6-9c3b-4abe-a2bb-831e6f47c168"
      },
      "source": [
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    output = []  \n",
        "    for i in range(len(words)-n+1):\n",
        "        output.append(words[i:i+n])\n",
        "    return output\n",
        "\n",
        "generate_ngrams('this is a sample text', 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLVqLtMA42Qt"
      },
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqhamRRX44dn",
        "outputId": "3f663434-9a76-4f8d-f9e2-e3d8b256f537"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "obj = TfidfVectorizer()\n",
        "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
        "X = obj.fit_transform(corpus)\n",
        "print (X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 1)\t0.34520501686496574\n",
            "  (0, 4)\t0.444514311537431\n",
            "  (0, 2)\t0.5844829010200651\n",
            "  (0, 7)\t0.5844829010200651\n",
            "  (1, 3)\t0.652490884512534\n",
            "  (1, 0)\t0.652490884512534\n",
            "  (1, 1)\t0.3853716274664007\n",
            "  (2, 5)\t0.5844829010200651\n",
            "  (2, 6)\t0.5844829010200651\n",
            "  (2, 1)\t0.34520501686496574\n",
            "  (2, 4)\t0.444514311537431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g_O-aZ77ayd"
      },
      "source": [
        "Word2Vec (can be used as feature vectors for ML model, used to measure text similarity using cosine similarity techniques, words clustering and text classification techniques.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDCCftZI7fB5",
        "outputId": "445b7dc5-1aa4-4406-e974-dde6267b950c"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
        "\n",
        "# train the model on your corpus  \n",
        "model = Word2Vec(sentences, min_count = 1)\n",
        "\n",
        "print (model.similarity('data', 'science'))\n",
        "\n",
        "\n",
        "print (model['learning'])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.020804897\n",
            "[ 3.3685784e-03 -1.6426287e-03 -2.4420028e-03  4.3364805e-03\n",
            " -2.4865146e-03  3.4459503e-03  4.3231943e-03  2.5872281e-03\n",
            " -1.0003664e-03  5.0351169e-04 -4.6071131e-03  4.7577024e-03\n",
            " -4.8981461e-04  1.1846402e-04  2.5968375e-03  4.4579855e-03\n",
            " -5.8399379e-04 -7.2048500e-04  1.8719595e-03  4.9485811e-03\n",
            " -6.8985455e-04  1.1082890e-03 -2.3890606e-03  4.2396975e-03\n",
            " -2.6134378e-03 -1.1951310e-03 -7.7792397e-04 -9.0145104e-04\n",
            " -1.3553821e-03 -3.0838964e-03 -1.6605931e-03 -8.2016236e-04\n",
            "  4.7121253e-03  1.5861172e-03 -2.4909395e-04  6.2848907e-04\n",
            " -2.7824426e-03  4.9023032e-03 -9.2682586e-04  6.5943657e-04\n",
            "  4.8098564e-03 -8.6111855e-04 -2.9137584e-03  4.0743177e-04\n",
            "  2.7494368e-03 -1.6205705e-03 -7.5914810e-04  4.7109206e-03\n",
            "  2.6546202e-03  3.2589834e-03 -1.6057199e-03  1.9310034e-03\n",
            " -3.4822368e-03  1.3980783e-03  3.2397606e-03 -3.4208449e-03\n",
            " -2.7833846e-03  1.7512117e-03  1.1960308e-03  4.1044033e-03\n",
            " -2.4341678e-03  1.1860195e-03 -7.1459642e-04  4.0471610e-03\n",
            "  2.0520508e-03 -2.3929556e-03  3.7089162e-04 -1.9575011e-04\n",
            "  2.4153918e-03  3.1093422e-03  8.8946981e-04 -4.3399432e-03\n",
            "  1.0722185e-03  4.9264673e-03 -1.9770903e-03  3.6917727e-03\n",
            " -1.0450011e-03  1.9908538e-03  3.4388190e-03  1.4439387e-03\n",
            "  9.1684939e-05 -3.5512261e-03 -2.6241643e-03  2.9719856e-03\n",
            "  1.8634882e-03 -1.0404750e-03  2.3417950e-03  8.8040280e-04\n",
            " -3.6126161e-03  3.3526637e-03 -2.2388652e-03 -9.9973578e-04\n",
            "  4.8908032e-03 -3.7389800e-03 -9.8953792e-04 -2.7206778e-03\n",
            " -2.1589582e-03 -3.7954960e-03  1.2435591e-03 -3.3028615e-03]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6_j-fGzG-Eq"
      },
      "source": [
        "Text Classification (Basic) -> Used Naive Bayes classifier. Formula :  Posterior = (Prior * Likelihood)/ Evidence (To know more : https://www.geeksforgeeks.org/naive-bayes-classifiers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "669FXcnBHBIA",
        "outputId": "b51c600f-17ff-4dd0-80d8-4d821506c6c9"
      },
      "source": [
        "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
        "from textblob import TextBlob\n",
        "training_corpus = [\n",
        "                   ('I am exhausted of this work.', 'Class_A'),\n",
        "                   (\"I can't cooperate with this\", 'Class_B'),\n",
        "                   ('He is my badest enemy!', 'Class_B'),\n",
        "                   ('My management is poor.', 'Class_B'),\n",
        "                   ('I love this burger.', 'Class_A'),\n",
        "                   ('This is an brilliant place!', 'Class_A'),\n",
        "                   ('I feel very good about these dates.', 'Class_A'),\n",
        "                   ('This is my best work.', 'Class_A'),\n",
        "                   (\"What an awesome view\", 'Class_A'),\n",
        "                   ('I do not like this dish', 'Class_B')]\n",
        "test_corpus = [\n",
        "                (\"I am not feeling well today.\", 'Class_B'), \n",
        "                (\"I feel brilliant!\", 'Class_A'), \n",
        "                ('Gary is a friend of mine.', 'Class_A'), \n",
        "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
        "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
        "\n",
        "model = NBC(training_corpus) \n",
        "print(model.classify(\"I have tried but cannot connect to SFTP\")) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class_B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ee3qYs4INZI"
      },
      "source": [
        "Accuracy of above approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KweTGnGIP8e",
        "outputId": "980efc1f-3554-431f-987c-688c97520ba9"
      },
      "source": [
        "print(model.accuracy(test_corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wy2kTafI_M5"
      },
      "source": [
        "Cosine Similarity (Maximum number of common words between compared sentences) \n",
        "(Can learn more about this : https://www.machinelearningplus.com/nlp/cosine-similarity/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG8OYBCgJA2G",
        "outputId": "86b5c57e-13ed-459b-a4c8-bbcf5727305b"
      },
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "def get_cosine(vec1, vec2):\n",
        "    common = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "   \n",
        "    if not denominator:\n",
        "        return 0.0 \n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "def text_to_vector(text): \n",
        "    words = text.split() \n",
        "    return Counter(words)\n",
        "\n",
        "text1 = 'I cannot connect to SFTP' \n",
        "text2 = 'I cannot connect to Git'\n",
        "\n",
        "vector1 = text_to_vector(text1) \n",
        "vector2 = text_to_vector(text2) \n",
        "cosine = get_cosine(vector1, vector2) \n",
        "print(cosine*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79.99999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}